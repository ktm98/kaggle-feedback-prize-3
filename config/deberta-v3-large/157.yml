wandb: True
competition: FB3

apex: True
debug: False
print_freq: 40
num_workers: 4
output_dir: ../result/deberta_large_157/
model: microsoft/deberta-v3-large
pretrained_path: None

gradient_checkpointing: True
scheduler: cosine  # linear, cosine
batch_scheduler: True
num_cycles: 0.5
# num_warmup_steps: 200
warmup_ratio: 0.0
epochs: 4
encoder_lr: 2.0e-5
decoder_lr: 2.0e-5
min_lr: 1.0e-6
eps: 1.0e-6
betas: [0.9, 0.999]
batch_size: 8
max_len: 1428
weight_decay: 0.01
gradient_accumulation_steps: 1
max_grad_norm: 1000
unscale: False
pooling: mean  # mean, attention
concat_layers: False

freeze_layers: 18

use_msd: False
n_msd: 7  # multi sample dropout
use_mixout: False
mixout_prob: 0.5530308654454139


reinit_layers: 0  # default: 0

weighted_layer_pooling: False
layer_start: 9  # 

use_prior_wd: False  # default: False

init_method: normal_  # xavier_normal_, kaiming_normal_, orthogonal_, normal_

lr_method: constant  # constant, decay, step
layerwise_lr_decay: 0.95  # noly for decay

add_new_token: True

use_pl: False
pl_exp_name: 
  - deberta_base_130
  # - deberta_base_102
  # - deberta_large_108
  # - deberta_large_109
pl_frac: 0.25  # plのデータから抽出される割合

# teacher_model: /root/result/deberta_base_130/

fgm: False
awp: False
adv_lr: 2.8547547523253952e-05
adv_eps: 4.543067954812687e-06
adv_start_epoch: 3

target_cols:
  - cohesion
  - syntax
  - vocabulary
  - phraseology
  - grammar
  - conventions

seed: 44
n_fold: 4
trn_fold:
  - 0
  - 1
  - 2
  - 3
  # - 4
  # - 5
  # - 6
  # - 7
  # - 8
  # - 9
  

use_unique_token: True
use_unique_token_as_target: False

train: True
# train_all: False